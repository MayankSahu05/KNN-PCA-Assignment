{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#KNN & PCA\n",
        " 1. — What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n",
        "\n",
        "Answer:\n",
        "K-Nearest Neighbors (KNN) is a non-parametric, instance-based supervised learning algorithm. It stores the training examples and, for a new query point, finds the k closest training examples (neighbors) according to a distance metric (commonly Euclidean).\n",
        "\n",
        "  - Classification: The predicted class for the query is typically the majority class among the k neighbors (optionally weighted by inverse distance).\n",
        "\n",
        " - Regression: The predicted value is typically the average (or distance-weighted average) of the target values of the k neighbors.\n",
        "\n",
        "   Key points:\n",
        "\n",
        " - No explicit training phase (lazy learner) — training = storing data.\n",
        "\n",
        " - Choice of k affects bias–variance: small k → low bias, high variance; large k → higher bias, lower variance.\n",
        "\n",
        " - Choice of distance metric (Euclidean, Manhattan, Minkowski) and feature scaling (StandardScaler) strongly influence performance.\n",
        "\n",
        " - Complexity: prediction cost is O(n_features × n_train) per query unless optimized (KD-tree, Ball-tree) — but those degrade in high dims.\n",
        "\n",
        "2. — What is the Curse of Dimensionality and how does it affect KNN performance?\n",
        "\n",
        "Answer:\n",
        "The curse of dimensionality refers to several problems that arise when the number of features (dimensions) grows large:\n",
        "\n",
        " - Distances become less informative: in high dimensions, distances between points concentrate (nearest and farthest distances become similar).\n",
        "\n",
        " - Sparsity: data become sparse, requiring exponentially more samples to densely cover the space.\n",
        "\n",
        " - Increased noise and overfitting risk.\n",
        "\n",
        "   For KNN: because KNN relies on distance, when distances lose discrimination, KNN struggles — neighbors may no longer be “meaningful,” and performance degrades. Dimensionality reduction (PCA, feature selection) or using distance metrics robust in high dims, and feature scaling, help.\n",
        "\n",
        "3. — What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
        "\n",
        "Answer:\n",
        "PCA is an unsupervised linear dimensionality reduction technique that finds orthogonal directions (principal components) of maximum variance in the data and projects data onto the top components. Steps: center data, compute covariance matrix, compute eigenvectors/eigenvalues, sort by eigenvalue, keep top components.\n",
        "\n",
        " - Difference from feature selection:\n",
        "\n",
        "    - PCA produces new features (linear combinations of original features) — feature extraction.\n",
        "\n",
        "   - Feature selection chooses a subset of original features (keeps original variables).\n",
        "\n",
        "   - PCA reduces dimensionality by capturing variance; selection keeps interpretable original features.\n",
        "\n",
        "4. — What are eigenvalues and eigenvectors in PCA, and why are they important?\n",
        "\n",
        "Answer:\n",
        "In PCA, eigenvectors of the covariance matrix are the principal component directions (unit vectors) — they indicate directions in feature space with maximal variance. Eigenvalues correspond to the variance explained by their eigenvectors (principal components). Sorting eigenvalues descending gives component importance. They are important because they let us:\n",
        "\n",
        " - Rank components by explained variance.\n",
        "\n",
        "  - Choose how many components to keep (e.g., retain components that collectively explain 90% variance).\n",
        "\n",
        " - Project high-dimensional data into a lower-dimensional subspace that preserves most variance.\n",
        "\n",
        "5. — How do KNN and PCA complement each other when applied in a single pipeline?\n",
        "\n",
        "Answer:\n",
        "PCA reduces dimensionality and noise, making distance metrics more meaningful and reducing computational cost for KNN. Pipeline: scale → PCA (reduce dims) → KNN. Benefits:\n",
        "\n",
        " - Mitigates curse of dimensionality.\n",
        "\n",
        " - Faster KNN predictions (fewer dimensions).\n",
        "\n",
        " - Often improves generalization by removing noisy / redundant features.\n",
        "\n",
        "   Caveat: PCA is linear and unsupervised — it may discard features relevant for classification if those have low variance. Consider supervised dimensionality reduction if needed.\n",
        "\n",
        " 6. Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases.\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "eIh27Yxi6P2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Without scaling\n",
        "knn_no_scale = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scale.fit(X_train, y_train)\n",
        "acc_no_scale = accuracy_score(y_test, knn_no_scale.predict(X_test))\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_s, y_train)\n",
        "acc_scaled = accuracy_score(y_test, knn_scaled.predict(X_test_s))\n",
        "\n",
        "print(\"Accuracy without scaling :\", round(acc_no_scale,4))\n",
        "print(\"Accuracy with scaling    :\", round(acc_scaled,4))\n"
      ],
      "metadata": {
        "id": "DnkYA9BZkgXy",
        "outputId": "6fcbdc73-a1b4-47d9-c7c2-ebf049dbb116",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling : 0.8056\n",
            "Accuracy with scaling    : 0.9722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component.\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "TyRw1FWjmIrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Standardize the dataset\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Fit PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Print explained variance ratio\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_, start=1):\n",
        "    print(f\"PC{i} : {ratio:.4f}\")\n",
        "\n",
        "print(\"\\nCumulative variance explained:\")\n",
        "print(pca.explained_variance_ratio_.cumsum())\n"
      ],
      "metadata": {
        "id": "92qabzOSofKK",
        "outputId": "e812770e-973d-4f37-b1a0-5be11b50bfe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PC1 : 0.3620\n",
            "PC2 : 0.1921\n",
            "PC3 : 0.1112\n",
            "PC4 : 0.0707\n",
            "PC5 : 0.0656\n",
            "PC6 : 0.0494\n",
            "PC7 : 0.0424\n",
            "PC8 : 0.0268\n",
            "PC9 : 0.0222\n",
            "PC10 : 0.0193\n",
            "PC11 : 0.0174\n",
            "PC12 : 0.0130\n",
            "PC13 : 0.0080\n",
            "\n",
            "Cumulative variance explained:\n",
            "[0.36198848 0.55406338 0.66529969 0.73598999 0.80162293 0.85098116\n",
            " 0.89336795 0.92017544 0.94239698 0.96169717 0.97906553 0.99204785\n",
            " 1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset.\n",
        "\n",
        "Answer:  "
      ],
      "metadata": {
        "id": "w5P9iBQ9phvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Scaled data reuse from above\n",
        "X_train_s, X_test_s, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# KNN on full scaled data\n",
        "knn_full = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_full.fit(X_train_s, y_train)\n",
        "acc_full = accuracy_score(y_test, knn_full.predict(X_test_s))\n",
        "\n",
        "# PCA with 2 components\n",
        "pca2 = PCA(n_components=2, random_state=42)\n",
        "X_pca2 = pca2.fit_transform(X_scaled)\n",
        "X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(\n",
        "    X_pca2, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "knn_pca2 = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca2.fit(X_train_p, y_train_p)\n",
        "acc_pca2 = accuracy_score(y_test_p, knn_pca2.predict(X_test_p))\n",
        "\n",
        "print(\"Accuracy (full scaled features):\", round(acc_full,4))\n",
        "print(\"Accuracy (PCA 2 components)   :\", round(acc_pca2,4))\n"
      ],
      "metadata": {
        "id": "bcwNw_uHp0AQ",
        "outputId": "ea6656a2-32de-4ddf-fe1c-a1dc1502e88e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (full scaled features): 0.9722\n",
            "Accuracy (PCA 2 components)   : 0.8889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.  Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results.\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "Yzxffd_xqBde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Use scaled data\n",
        "X_train_s, X_test_s, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Euclidean distance\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_s, y_train)\n",
        "acc_euclidean = accuracy_score(y_test, knn_euclidean.predict(X_test_s))\n",
        "\n",
        "# Manhattan distance\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_s, y_train)\n",
        "acc_manhattan = accuracy_score(y_test, knn_manhattan.predict(X_test_s))\n",
        "\n",
        "print(\"Euclidean accuracy :\", round(acc_euclidean,4))\n",
        "print(\"Manhattan accuracy :\", round(acc_manhattan,4))\n"
      ],
      "metadata": {
        "id": "gVSCuP_nqN0M",
        "outputId": "245677c4-b4ca-4b15-9a68-3e50a7bbf5c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Euclidean accuracy : 0.9722\n",
            "Manhattan accuracy : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "Explain how you would:\n",
        "\n",
        "● Use PCA to reduce dimensionality\n",
        "\n",
        "● Decide how many components to keep\n",
        "\n",
        "\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "\n",
        "● Evaluate the model\n",
        "\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data"
      ],
      "metadata": {
        "id": "ZmwezvoqqaFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import balanced_accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Simulate high-dimensional gene data\n",
        "X_hd, y_hd = make_classification(\n",
        "    n_samples=100, n_features=5000, n_informative=50, n_redundant=50,\n",
        "    n_classes=3, random_state=42\n",
        ")\n",
        "\n",
        "X_train_hd, X_test_hd, y_train_hd, y_test_hd = train_test_split(\n",
        "    X_hd, y_hd, test_size=0.2, random_state=42, stratify=y_hd\n",
        ")\n",
        "\n",
        "# Pipeline: StandardScaler -> PCA -> KNN\n",
        "pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA()),\n",
        "    ('knn', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "# Parameter grid\n",
        "param_grid = {\n",
        "    'pca__n_components': [10, 20, 50, 100],\n",
        "    'knn__n_neighbors': [3, 5, 7],\n",
        "    'knn__metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "grid = GridSearchCV(pipe, param_grid=param_grid, cv=cv,\n",
        "                    scoring='balanced_accuracy', n_jobs=-1)\n",
        "grid.fit(X_train_hd, y_train_hd)\n",
        "\n",
        "print(\"Best parameters:\", grid.best_params_)\n",
        "print(\"Best cross-val balanced accuracy:\", round(grid.best_score_,4))\n",
        "\n",
        "# Evaluate on test set\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test_hd)\n",
        "\n",
        "print(\"\\nClassification Report on Test Data:\\n\")\n",
        "print(classification_report(y_test_hd, y_pred))\n"
      ],
      "metadata": {
        "id": "HwmGKEjrq811",
        "outputId": "35004309-89af-40ef-e59b-c520e02251bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "30 fits failed out of a total of 120.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "30 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/pipeline.py\", line 654, in fit\n",
            "    Xt = self._fit(X, y, routed_params, raw_params=params)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/pipeline.py\", line 588, in _fit\n",
            "    X, fitted_transformer = fit_transform_one_cached(\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/joblib/memory.py\", line 326, in __call__\n",
            "    return self.func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/pipeline.py\", line 1551, in _fit_transform_one\n",
            "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_set_output.py\", line 319, in wrapped\n",
            "    data_to_wrap = f(self, X, *args, **kwargs)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/decomposition/_pca.py\", line 468, in fit_transform\n",
            "    U, S, _, X, x_is_centered, xp = self._fit(X)\n",
            "                                    ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/decomposition/_pca.py\", line 542, in _fit\n",
            "    return self._fit_full(X, n_components, xp, is_array_api_compliant)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/decomposition/_pca.py\", line 556, in _fit_full\n",
            "    raise ValueError(\n",
            "ValueError: n_components=100 must be between 0 and min(n_samples, n_features)=64 with svd_solver='full'\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.41777778 0.31333333 0.34              nan 0.31111111 0.34444444\n",
            " 0.37333333        nan 0.39333333 0.33555556 0.32444444        nan\n",
            " 0.30666667 0.44888889 0.34444444        nan 0.32222222 0.33555556\n",
            " 0.34              nan 0.37111111 0.33333333 0.34              nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'knn__metric': 'manhattan', 'knn__n_neighbors': 3, 'pca__n_components': 20}\n",
            "Best cross-val balanced accuracy: 0.4489\n",
            "\n",
            "Classification Report on Test Data:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      1.00      0.54         7\n",
            "           1       1.00      0.17      0.29         6\n",
            "           2       0.00      0.00      0.00         7\n",
            "\n",
            "    accuracy                           0.40        20\n",
            "   macro avg       0.46      0.39      0.27        20\n",
            "weighted avg       0.43      0.40      0.27        20\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}